{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Anatomy of a Quantizer\n",
    "\n",
    "## What's in a Quantizer? \n",
    "\n",
    "In a broad sense, a quantizer is anything that implements a quantization technique, and the flexibility of Brevitas means that there are different ways to do so.  \n",
    "However, to keep our terminology straight, we refer to a quantizer as a specific kind of way to implement quantization, the one preferred and adopted by default. That is, a quantizer is a subclass of a `brevitas.inject.ExtendedInjector` that carries a `tensor_quant` attribute.  \n",
    "We have seen in previous tutorials quantizers being imported from `brevitas.quant` and passed on to quantized layers. We can easily very what we just said on one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.inject import ExtendedInjector\n",
    "from brevitas.quant.scaled_int import Int8ActPerTensorFloat \n",
    "\n",
    "issubclass(Int8ActPerTensorFloat, ExtendedInjector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RescalingIntQuant(\n",
       "  (int_quant): IntQuant(\n",
       "    (float_to_int_impl): RoundSte()\n",
       "    (tensor_clamp_impl): TensorClamp()\n",
       "    (delay_wrapper): DelayWrapper(\n",
       "      (delay_impl): _NoDelay()\n",
       "    )\n",
       "  )\n",
       "  (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "    (stats_input_view_shape_impl): OverTensorView()\n",
       "    (stats): _Stats(\n",
       "      (stats_impl): AbsPercentile()\n",
       "    )\n",
       "    (restrict_clamp_scaling): _RestrictClampValue(\n",
       "      (clamp_min_ste): Identity()\n",
       "      (restrict_value_impl): FloatRestrictValue()\n",
       "    )\n",
       "    (restrict_inplace_preprocess): Identity()\n",
       "  )\n",
       "  (int_scaling_impl): IntScaling()\n",
       "  (zero_point_impl): ZeroZeroPoint(\n",
       "    (zero_point): StatelessBuffer()\n",
       "  )\n",
       "  (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "    (bit_width): StatelessBuffer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Int8ActPerTensorFloat.tensor_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we said *subclass* and not *instance*. To understand why that's the case, we have to understand what an `ExtendedInjector` is and why it's used in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization with auto-wiring Dependency Injection\n",
    "\n",
    "Pytorch has exploded in popularity thanks to its straightforward numpy-like *define-by-run* execution model. However, when it comes to applying quantization, this style of programming poses a problem.  \n",
    "\n",
    "Many quantization methods depend on making decisions based on the (in Pytorch terms) `state_dict` of the original floating-point model to finetune with quantization. However, when we instantiate a model in Pytorch we can't know on the spot if a state_dict is going to be loaded a few lines of code later or not. Yet, because Pytorch is define-by-run, we need our model to work consistently both before and after a `state_dict` is possibly loaded. In a traditional scenario that wouldn't pose a problem. However, with quantization in the loop, the way a quantizer is defined might change before and after a pretrained `state_dict` is loaded.\n",
    "\n",
    "That means that we need a way to define our quantized model such that it can react appropriately in case the `state_dict` changes. In a Python-only world that wouldn't be too hard. However, in order to mitigate the performance impact of quantization-aware training, Brevitas makes extended use of Pytorch's JIT compiler for a custom subset of Python, TorchScript. That means that in most scenarios, when a `state_dict` is loaded, we need to recompile parts of the model. Because compilation in general is a lossy process, a TorchScript component cannot simply re-compile itself based on new input information. \n",
    "\n",
    "We need then a way to *declare* a quantization method such that it can be re-initialized and JIT compiled any time the `state_dict` changes. Because we want to support arbitrarly-complex user-defined quantization algorithms, this method has to be generic, i.e. it cannot depend on the specifics of the quantization algorithm implemented.\n",
    "\n",
    "Implementing a quantizer with an `ExtendedInjector` is a way to do so. Specifically, an `ExtendedInjector` extends an `Injector` from an older version (0.2.1) of the excellent dependency-injection *[dependencies](https://github.com/proofit404/dependencies)* library with support for a couple of extra features that are specific to Brevitas' needs.\n",
    "\n",
    "An `Injector` (and an `ExtendedInjector`) allows to take what might be a very complicated graph of interwined objects and turns it into a flat list of variables that are capable of auto-assembly by matching variable names to arguments names. This technique typically goes under the name of auto-wiring dependency injection. \n",
    "\n",
    "In the context of Brevitas, the goal is gather all the modules and hyperparameters that contribute to a quantization implementation such that they can be re-assembled automatically on demand. What comes out of this process is a `tensor_quant` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Practical Example: Binary Quantization\n",
    "\n",
    "To make things practical, let's look at how we can implement a simple variant of binary quantization. All the components typically used to implement quantization can be found under `brevitas.core`. As mentioned before, Brevitas makes heavy use of TorchScript. In particular, all the components found under `brevitas.core` are implemented as `ScriptModule` that can be assembled together.\n",
    "The core `ScriptModule` that implements binarization can be found under `brevitas.core.quant`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def pretty_print_source(source):\n",
    "    display(Markdown('```python\\n' + source + '\\n```'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "class BinaryQuant(brevitas.jit.ScriptModule):\n",
       "    \"\"\"\n",
       "    ScriptModule that implements scaled uniform binary quantization of an input tensor.\n",
       "    Quantization is performed with :func:`~brevitas.function.ops_ste.binary_sign_ste`.\n",
       "\n",
       "    Args:\n",
       "        scaling_impl (Module): Module that returns a scale factor.\n",
       "        quant_delay_steps (int): Number of training steps to delay quantization for. Default: 0\n",
       "\n",
       "    Returns:\n",
       "        Tuple[Tensor, Tensor, Tensor, Tensor]: Quantized output in de-quantized format, scale,\n",
       "            zero-point, bit_width.\n",
       "\n",
       "    Examples:\n",
       "        >>> from brevitas.core.scaling import ConstScaling\n",
       "        >>> binary_quant = BinaryQuant(ConstScaling(0.1))\n",
       "        >>> inp = torch.Tensor([0.04, -0.6, 3.3])\n",
       "        >>> out, scale, zero_point, bit_width = binary_quant(inp)\n",
       "        >>> out\n",
       "        tensor([ 0.1000, -0.1000,  0.1000])\n",
       "        >>> scale\n",
       "        tensor(0.1000)\n",
       "        >>> zero_point\n",
       "        tensor(0.)\n",
       "        >>> bit_width\n",
       "        tensor(1.)\n",
       "\n",
       "    Note:\n",
       "        Maps to quant_type == QuantType.BINARY == 'BINARY' == 'binary' when applied to weights\n",
       "         in higher-level APIs.\n",
       "\n",
       "    Note:\n",
       "        Set env variable BREVITAS_JIT=1 to enable TorchScript compilation of this module.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, scaling_impl: Module, quant_delay_steps: int = 0):\n",
       "        super(BinaryQuant, self).__init__()\n",
       "        self.scaling_impl = scaling_impl\n",
       "        self.bit_width = BitWidthConst(1)\n",
       "        self.zero_point = StatelessBuffer(torch.tensor(0.0))\n",
       "        self.delay_wrapper = DelayWrapper(quant_delay_steps)\n",
       "\n",
       "    @brevitas.jit.script_method\n",
       "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
       "        scale = self.scaling_impl(x)\n",
       "        y = binary_sign_ste(x) * scale\n",
       "        y = self.delay_wrapper(x, y)\n",
       "        return y, scale, self.zero_point(), self.bit_width()\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from brevitas.core.quant import BinaryQuant\n",
    "\n",
    "source = inspect.getsource(BinaryQuant)  \n",
    "pretty_print_source(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation is quite simple. Apart from `quant_delay_steps`, which allows to delay quantization by a certain number of training steps (*default = 0*), the only other argument that BinaryQuant accepts is an implementation to compute the scale factor. `bit_width` is fixed to 1 and `zero-point` is fixed to 0.\n",
    "\n",
    "We pick as scale factor implementation a `ScriptModule` called `ParameterScaling`, which implements a learned parameter with user-defined initialization. It can be found under `brevitas.core.scaling`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.core.scaling import ParameterScaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Binary Quantization\n",
    "\n",
    "As a first step, we simply instantiate `BinaryQuant` with `ParameterScaling` using `scaling_init` equal *0.1* and we call it on a random floating-point input tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1000,  0.1000, -0.1000, -0.1000],\n",
       "         [ 0.1000,  0.1000,  0.1000,  0.1000],\n",
       "         [ 0.1000,  0.1000, -0.1000,  0.1000],\n",
       "         [ 0.1000, -0.1000,  0.1000,  0.1000]], grad_fn=<MulBackward0>),\n",
       " tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>),\n",
       " tensor(0.),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "manual_tensor_quant = BinaryQuant(scaling_impl=ParameterScaling(scaling_init=0.1))\n",
    "manual_tensor_quant(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing too surprising here, as expected the tensor is binarized with the scale factor we defined. Note however how `manual_tensor_quant` is returning a `tuple` and not a `QuantTensor`. This is because support for custom data structures in TorchScript is still quite limited, so `QuantTensor` are allocated only in Python-world abstractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Quantization with an ExtendedInjector\n",
    "\n",
    "Let's now declare `tensor_quant` through an `ExtendedInjector`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1000,  0.1000, -0.1000,  0.1000],\n",
       "         [ 0.1000,  0.1000,  0.1000, -0.1000],\n",
       "         [-0.1000, -0.1000, -0.1000,  0.1000],\n",
       "         [ 0.1000, -0.1000, -0.1000,  0.1000]], grad_fn=<MulBackward0>),\n",
       " tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>),\n",
       " tensor(0.),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.inject import ExtendedInjector\n",
    "\n",
    "class MyBinaryQuantizer(ExtendedInjector):\n",
    "    tensor_quant = BinaryQuant\n",
    "    scaling_impl=ParameterScaling\n",
    "    scaling_init=0.1\n",
    "\n",
    "inj_tensor_quant = MyBinaryQuantizer.tensor_quant\n",
    "inj_tensor_quant(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time `MyBinaryQuantizer.tensor_quant` is called, a new instance of `BinaryQuant` is created. Note how the attributes of `MyBinaryQuantizer` are designed to match the name of the arguments of each other, except for `tensor_quant`, which is what we are interested in retrieving from the outside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inheritance and Composition of Quantizers\n",
    "\n",
    "The advantage of expressing a quantizer through a Python class also means that we can leverage both *inheritance* and *composition*. So for example we can inherit from `MyBinaryQuantizer` and override `scaling_init` with a new value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1., -1.,  1., -1.],\n",
       "         [ 1., -1., -1.,  1.],\n",
       "         [ 1., -1.,  1.,  1.],\n",
       "         [-1., -1.,  1., -1.]], grad_fn=<MulBackward0>),\n",
       " tensor(1., grad_fn=<AbsBinarySignGradFnBackward>),\n",
       " tensor(0.),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyChildBinaryQuantizer(MyBinaryQuantizer):\n",
    "    scaling_init=1.0\n",
    "    \n",
    "child_inj_tensor_quant = MyChildBinaryQuantizer.tensor_quant\n",
    "child_inj_tensor_quant(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can leverage composition by assembling together various classes containing different pieces of a quantizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1000,  0.1000, -0.1000, -0.1000,  0.1000,  0.1000,  0.1000,  0.1000,\n",
       "           0.1000,  0.1000],\n",
       "         [-0.1000, -0.1000,  0.1000,  0.1000, -0.1000,  0.1000, -0.1000, -0.1000,\n",
       "           0.1000,  0.1000],\n",
       "         [ 0.1000, -0.1000, -0.1000,  0.1000,  0.1000, -0.1000,  0.1000, -0.1000,\n",
       "          -0.1000, -0.1000],\n",
       "         [-0.1000, -0.1000, -0.1000, -0.1000, -0.1000,  0.1000, -0.1000,  0.1000,\n",
       "          -0.1000, -0.1000],\n",
       "         [ 0.1000,  0.1000, -0.1000,  0.1000,  0.1000,  0.1000, -0.1000,  0.1000,\n",
       "           0.1000, -0.1000],\n",
       "         [ 0.1000, -0.1000, -0.1000, -0.1000,  0.1000, -0.1000, -0.1000, -0.1000,\n",
       "           0.1000,  0.1000],\n",
       "         [ 0.1000, -0.1000, -0.1000,  0.1000,  0.1000, -0.1000,  0.1000,  0.1000,\n",
       "           0.1000,  0.1000],\n",
       "         [-0.1000, -0.1000, -0.1000, -0.1000,  0.1000,  0.1000, -0.1000, -0.1000,\n",
       "           0.1000,  0.1000],\n",
       "         [-0.1000, -0.1000,  0.1000, -0.1000, -0.1000, -0.1000,  0.1000, -0.1000,\n",
       "           0.1000,  0.1000],\n",
       "         [-0.1000,  0.1000,  0.1000, -0.1000, -0.1000, -0.1000, -0.1000, -0.1000,\n",
       "          -0.1000, -0.1000]], grad_fn=<MulBackward0>),\n",
       " tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>),\n",
       " tensor(0.),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyBinaryImpl(ExtendedInjector):\n",
    "    tensor_quant = BinaryQuant\n",
    "\n",
    "class MyScalingImpl(ExtendedInjector):\n",
    "    scaling_impl=ParameterScaling\n",
    "    scaling_init=0.1\n",
    "    \n",
    "class MyComposedBinaryQuantizer(MyBinaryImpl, MyScalingImpl):\n",
    "    pass\n",
    "\n",
    "comp_inj_tensor_quant = MyComposedBinaryQuantizer.tensor_quant\n",
    "comp_inj_tensor_quant(torch.randn(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing a Custom Quantizer to QuantConv2d\n",
    "\n",
    "As expected, we can pass the quantizer to a quantized layer such as QuantConv2d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[ 0.1000, -0.1000,  0.1000],\n",
       "          [ 0.1000, -0.1000,  0.1000],\n",
       "          [-0.1000, -0.1000,  0.1000]],\n",
       "\n",
       "         [[-0.1000, -0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000, -0.1000],\n",
       "          [ 0.1000,  0.1000, -0.1000]],\n",
       "\n",
       "         [[ 0.1000,  0.1000, -0.1000],\n",
       "          [-0.1000, -0.1000, -0.1000],\n",
       "          [-0.1000,  0.1000, -0.1000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000]],\n",
       "\n",
       "         [[-0.1000, -0.1000, -0.1000],\n",
       "          [ 0.1000, -0.1000, -0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000]],\n",
       "\n",
       "         [[ 0.1000, -0.1000, -0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000],\n",
       "          [ 0.1000,  0.1000,  0.1000]]]], grad_fn=<MulBackward0>), scale=tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed=None, training=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.nn import QuantConv2d\n",
    "\n",
    "binary_weight_quant_conv = QuantConv2d(3, 2, (3,3), weight_quant=MyBinaryQuantizer)\n",
    "quant_weight = binary_weight_quant_conv.quant_weight()\n",
    "quant_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note however how the `QuantTensor` is not properly formed, as the `signed` attribute is `None`. This means that `quant_weight` is not considered valid, as the affine quantization invariant cannot be computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_weight.is_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`signed` is one of those attributes that in the case of binary quantization has to be explicitly defined by the user. We can do so by simply setting it in the quantizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.1000, -0.1000, -0.1000],\n",
       "          [ 0.1000,  0.1000, -0.1000],\n",
       "          [ 0.1000, -0.1000,  0.1000]],\n",
       "\n",
       "         [[-0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000, -0.1000,  0.1000],\n",
       "          [ 0.1000, -0.1000,  0.1000]],\n",
       "\n",
       "         [[-0.1000,  0.1000, -0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000, -0.1000]]],\n",
       "\n",
       "\n",
       "        [[[-0.1000, -0.1000, -0.1000],\n",
       "          [-0.1000,  0.1000, -0.1000],\n",
       "          [ 0.1000, -0.1000, -0.1000]],\n",
       "\n",
       "         [[-0.1000, -0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000, -0.1000, -0.1000]],\n",
       "\n",
       "         [[-0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000, -0.1000, -0.1000]]]], grad_fn=<MulBackward0>), scale=tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed=True, training=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySignedBinaryQuantizer(MyBinaryQuantizer):\n",
    "    signed = True\n",
    "    \n",
    "binary_weight_quant_conv = QuantConv2d(3, 2, (3,3), weight_quant=MySignedBinaryQuantizer)\n",
    "signed_quant_weight = binary_weight_quant_conv.quant_weight()\n",
    "signed_quant_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signed_quant_weight.is_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the quant weights are valid.\n",
    "\n",
    "When we want to add or override an single attribute of a quantizer passed to a layer, defining a whole new quantizer can be too verbose. There is a simpler syntax to achieve the same goal. Let's say we want to have `scaling_init = 0.0001`. We can simply do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.0010,  0.0010, -0.0010],\n",
       "          [-0.0010, -0.0010, -0.0010],\n",
       "          [-0.0010,  0.0010, -0.0010]],\n",
       "\n",
       "         [[ 0.0010, -0.0010, -0.0010],\n",
       "          [-0.0010,  0.0010, -0.0010],\n",
       "          [-0.0010,  0.0010, -0.0010]],\n",
       "\n",
       "         [[ 0.0010, -0.0010, -0.0010],\n",
       "          [-0.0010, -0.0010,  0.0010],\n",
       "          [ 0.0010,  0.0010,  0.0010]]],\n",
       "\n",
       "\n",
       "        [[[-0.0010,  0.0010,  0.0010],\n",
       "          [ 0.0010,  0.0010, -0.0010],\n",
       "          [-0.0010, -0.0010,  0.0010]],\n",
       "\n",
       "         [[-0.0010, -0.0010,  0.0010],\n",
       "          [-0.0010,  0.0010, -0.0010],\n",
       "          [ 0.0010,  0.0010, -0.0010]],\n",
       "\n",
       "         [[-0.0010, -0.0010,  0.0010],\n",
       "          [ 0.0010,  0.0010,  0.0010],\n",
       "          [-0.0010, -0.0010, -0.0010]]]], grad_fn=<MulBackward0>), scale=tensor(0.0010, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed=True, training=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_scale_quant_conv = QuantConv2d(3, 2, (3,3), weight_quant=MySignedBinaryQuantizer, weight_scaling_init=0.001)\n",
    "small_scale_quant_conv.quant_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did was to take the name of the attribute `scaling_init`, add the prefix `weight_`, and pass it as a keyword argument to `QuantConv2d`. What happens in the background is that the keyword arguments prefixed with `weight_` are set as attributes of `weight_quant`, possibly overriding any pre-existing value. The same principle applies to `input_`, `output_` and `bias_`. In case the corrisponding quantizer is `None`, a new *empty* `ExtendedInjector` is internally allocated and attributes are passed to that. So for example we can define `output_quant` (which by default is None) completely *inline* without defining an explicit standalone quantizer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1., -1.,  1.],\n",
       "          [-1.,  1., -1.],\n",
       "          [ 1.,  1., -1.]],\n",
       "\n",
       "         [[ 1., -1.,  1.],\n",
       "          [-1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.]]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inline_output_quant = QuantConv2d(\n",
    "    3, 2, (3,3), \n",
    "    weight_quant=MySignedBinaryQuantizer,\n",
    "    output_tensor_quant = BinaryQuant,\n",
    "    output_scaling_impl=ParameterScaling,\n",
    "    output_scaling_init=1.0)\n",
    "\n",
    "inline_output_quant(torch.randn(1, 3, 5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the reason why, as it was mentioned in the first tutorial, quantized layers can accept arbitrary keyword arguments. It's really just a way to support different styles of syntax when defining a quantizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing a custom quantizer to QuantReLU\n",
    "\n",
    "We can do the same thing with quantized activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.1000, 0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.nn import QuantReLU\n",
    "\n",
    "binary_relu = QuantReLU(act_quant=MySignedBinaryQuantizer)\n",
    "binary_relu(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there isn't really much difference between a quantizer for weights and a quantizer for activations. The difference with activations is that a prefix is not required when passing keyword arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0010, 0.0010, 0.0010, 0.0010],\n",
       "        [0.0010, 0.0010, 0.0010, 0.0010],\n",
       "        [0.0010, 0.0010, 0.0010, 0.0010],\n",
       "        [0.0010, 0.0010, 0.0010, 0.0010]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_scale_binary_relu = QuantReLU(act_quant=MySignedBinaryQuantizer, scaling_init=0.001)\n",
    "small_scale_binary_relu(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Custom Quantizer initialized with Weight Statistics\n",
    "\n",
    "So far we have seen use-cases where an `ExtendedInjector` provides at best a different kind of syntax to define a quantizer, without any particular other advantage. Let's now make things a bit more complicated to show the sort of situations where it really shines. \n",
    "\n",
    "Let's say we want to define a binary weight quantizer where `scaling_impl` is still `ParameterScaling`. However, instead of being user-defined, we want `scaling_init` to be the maximum value found in the weight tensor of the quantized layer.\n",
    "To support this sort of use cases where the quantizer depends on the layer, a quantized layer automatically passes itself to all its quantizers under the name of `module`. With some machinery then, we can achieve our goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.inject import value\n",
    "\n",
    "class ParamFromMaxWeightQuantizer(MySignedBinaryQuantizer):\n",
    "    \n",
    "    @value\n",
    "    def scaling_init(module):\n",
    "        return module.weight.abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we are leveraging the `@value` *decorator* to define a function that is executed at *dependency-injection (DI) time*. This kind of behaviour is similar to defining a @property instead of an attribute, with the difference that a @value function can depend on other attributes of the Injector, which are automatically passed in as arguments of the function during DI. \n",
    "\n",
    "Let's now pass the quantizer to a QuantConv2d and retrieve its quantized weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.1848, -0.1848, -0.1848],\n",
       "          [ 0.1848,  0.1848, -0.1848],\n",
       "          [ 0.1848,  0.1848,  0.1848]],\n",
       "\n",
       "         [[-0.1848,  0.1848,  0.1848],\n",
       "          [ 0.1848, -0.1848, -0.1848],\n",
       "          [ 0.1848, -0.1848,  0.1848]],\n",
       "\n",
       "         [[-0.1848,  0.1848,  0.1848],\n",
       "          [-0.1848,  0.1848,  0.1848],\n",
       "          [-0.1848, -0.1848, -0.1848]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1848, -0.1848,  0.1848],\n",
       "          [-0.1848, -0.1848,  0.1848],\n",
       "          [ 0.1848, -0.1848,  0.1848]],\n",
       "\n",
       "         [[ 0.1848, -0.1848,  0.1848],\n",
       "          [ 0.1848, -0.1848, -0.1848],\n",
       "          [-0.1848, -0.1848,  0.1848]],\n",
       "\n",
       "         [[-0.1848, -0.1848, -0.1848],\n",
       "          [ 0.1848,  0.1848, -0.1848],\n",
       "          [ 0.1848, -0.1848, -0.1848]]]], grad_fn=<MulBackward0>), scale=tensor(0.1848, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed=True, training=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_from_max_quant_conv = QuantConv2d(3, 2, (3, 3), weight_quant=ParamFromMaxWeightQuantizer)\n",
    "param_from_max_quant_conv.quant_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed we can verify that `quant_weight_scale()` is equal to `weight.abs().max()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(param_from_max_quant_conv.quant_weight_scale() == param_from_max_quant_conv.weight.abs().max()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say now that we want to load a pretrained floating-point weight tensor on top of our quantized model. We simuate this scenario by defining a separate `nn.Conv2d` layer with the same weight shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1913, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "float_conv = nn.Conv2d(3, 2, (3, 3))\n",
    "float_conv.weight.abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we load it on top of `param_from_max_quant_conv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for QuantConv2d:\n\tMissing key(s) in state_dict: \"weight_quant.tensor_quant.scaling_impl.value\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5b3646241211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparam_from_max_quant_conv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat_conv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\pytorch_latest\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1052\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1053\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for QuantConv2d:\n\tMissing key(s) in state_dict: \"weight_quant.tensor_quant.scaling_impl.value\". "
     ]
    }
   ],
   "source": [
    "param_from_max_quant_conv.load_state_dict(float_conv.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, we get an error. This is because `ParameterScaling` contains a learned parameter, and Pytorch expects all learned parameters of a model to be contained in a state dict.  \n",
    "We can work around the issue by either setting an appropriate config flag in Brevitas, or by passing `strict=False` to load_state_dict. We go with the first way as setting `strict=False` is too forgiving to other kind of problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas import config\n",
    "config.IGNORE_MISSING_KEYS = True\n",
    "\n",
    "param_from_max_quant_conv.load_state_dict(float_conv.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have also achieve the same goal by setting the *env variable* `BREVITAS_IGNORE_MISSING_KEYS=1`.\n",
    "\n",
    "And now we can look at the quantized weights again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[ 0.1913,  0.1913, -0.1913],\n",
       "          [-0.1913, -0.1913,  0.1913],\n",
       "          [-0.1913, -0.1913,  0.1913]],\n",
       "\n",
       "         [[-0.1913, -0.1913, -0.1913],\n",
       "          [ 0.1913,  0.1913,  0.1913],\n",
       "          [-0.1913,  0.1913,  0.1913]],\n",
       "\n",
       "         [[ 0.1913,  0.1913,  0.1913],\n",
       "          [-0.1913,  0.1913, -0.1913],\n",
       "          [-0.1913, -0.1913,  0.1913]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1913,  0.1913,  0.1913],\n",
       "          [ 0.1913,  0.1913,  0.1913],\n",
       "          [-0.1913, -0.1913, -0.1913]],\n",
       "\n",
       "         [[-0.1913, -0.1913,  0.1913],\n",
       "          [-0.1913,  0.1913, -0.1913],\n",
       "          [-0.1913,  0.1913, -0.1913]],\n",
       "\n",
       "         [[-0.1913, -0.1913, -0.1913],\n",
       "          [ 0.1913, -0.1913, -0.1913],\n",
       "          [-0.1913,  0.1913,  0.1913]]]], grad_fn=<MulBackward0>), scale=tensor(0.1913, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed=True, training=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_from_max_quant_conv.quant_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the scale factor has been updated to the new `weight.abs().max()`. \n",
    "\n",
    "What happens internally is that after `load_state_dict` is called on the layer, `ParamFromMaxWeightQuantizer.tensor_quant` gets called again to re-initialize `BinaryQuant`, and in turn `ParameterScaling` is re-initialized with a new `scaling_init` value computed based on the updated `module.weight` tensor. This whole process wouldn't have been possible without an `ExtendedInjector` behind it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom quantization API\n",
    "\n",
    "Now let's make things even more complicated. We are going to look at a scenario that illustrates the differences between a standard `Injector` (implemented in the dependencies library) and our `ExtendedInjector` extension.  \n",
    "\n",
    "Let's say we want to build two quantizers for respectively weights and activations and build a simple API on top of them. In particular, we want to be able to switch between `BinaryQuant` and `ClampedBinaryQuant` (a variant of binary quantization with clamping typically used in activation quantization), and we want to optionally perform *per-channel scaling*. \n",
    "We can go as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.core.quant import ClampedBinaryQuant\n",
    "from brevitas.inject import this\n",
    "\n",
    "\n",
    "class CommonQuantizer(ExtendedInjector):\n",
    "    scaling_impl = ParameterScaling\n",
    "    signed=True\n",
    "    \n",
    "    @value\n",
    "    def tensor_quant(is_clamped):\n",
    "        # returning a class to auto-wire from a value function\n",
    "        # wouldn't be allowed in a standard Injector\n",
    "        if is_clamped:\n",
    "            return ClampedBinaryQuant\n",
    "        else:\n",
    "            return BinaryQuant\n",
    "    \n",
    "    @value\n",
    "    def scaling_shape(scaling_per_output_channel):\n",
    "        if scaling_per_output_channel:\n",
    "            # returning this.something from a value function \n",
    "            # wouldn't be allowed in a standard Injector\n",
    "            return this.per_channel_broadcastable_shape\n",
    "        else:\n",
    "            return ()\n",
    "        \n",
    "        \n",
    "class AdvancedWeightQuantizer(CommonQuantizer):\n",
    "        \n",
    "    @value\n",
    "    def per_channel_broadcastable_shape(module):\n",
    "        return (module.weight.shape[0], 1, 1, 1)\n",
    "    \n",
    "    @value\n",
    "    def scaling_init(module, scaling_per_output_channel):\n",
    "        if scaling_per_output_channel:\n",
    "            num_ch = module.weight.shape[0]\n",
    "            return module.weight.abs().view(num_ch, -1).max(dim=1)[0].view(-1, 1, 1, 1)\n",
    "        else:\n",
    "            return module.weight.abs().max()\n",
    "        \n",
    "        \n",
    "class AdvancedActQuantizer(CommonQuantizer):\n",
    "    scaling_init = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bunch of things going on here to unpack.  \n",
    "\n",
    "The first one is that a `@value` function can return a class to auto-wire and inject, as seen in the definition of `tensor_quant`. This wouldn't normally be possible with a standard `Injector`, but it's possible with an `ExtendedInjector`. This way we can switch between different implementations of `tensor_quant` with a boolean flag.  \n",
    "\n",
    "The second one is the special object `this`. `this` is already present in the *dependencies* library, and it's used as a way to retrieve attributes of the quantizer from within the quantizer itself. However, normally it wouldn't be possible to return a reference to `this` from a `@value` function. Again this is something that only a `ExtendedInjector` supports, and it allows to chain different attributes in a way such that the chained values are computed only when necessary. \n",
    "\n",
    "Let's see the quantizers applied to a layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[ 0.1718, -0.1718, -0.1718],\n",
       "          [ 0.1718, -0.1718, -0.1718],\n",
       "          [-0.1718,  0.1718, -0.1718]],\n",
       "\n",
       "         [[-0.1718,  0.1718, -0.1718],\n",
       "          [-0.1718, -0.1718, -0.1718],\n",
       "          [ 0.1718,  0.1718, -0.1718]],\n",
       "\n",
       "         [[-0.1718,  0.1718,  0.1718],\n",
       "          [-0.1718, -0.1718, -0.1718],\n",
       "          [ 0.1718, -0.1718, -0.1718]]],\n",
       "\n",
       "\n",
       "        [[[-0.1897, -0.1897,  0.1897],\n",
       "          [ 0.1897,  0.1897,  0.1897],\n",
       "          [-0.1897, -0.1897,  0.1897]],\n",
       "\n",
       "         [[-0.1897, -0.1897,  0.1897],\n",
       "          [ 0.1897,  0.1897, -0.1897],\n",
       "          [-0.1897, -0.1897, -0.1897]],\n",
       "\n",
       "         [[-0.1897, -0.1897, -0.1897],\n",
       "          [ 0.1897, -0.1897,  0.1897],\n",
       "          [-0.1897,  0.1897,  0.1897]]]], grad_fn=<MulBackward0>), scale=tensor([[[[0.1718]]],\n",
       "\n",
       "\n",
       "        [[[0.1897]]]], grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed=True, training=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_channel_quant_conv = QuantConv2d(\n",
    "    3, 2, (3, 3), \n",
    "    weight_quant=AdvancedWeightQuantizer, \n",
    "    weight_is_clamped=False, \n",
    "    weight_scaling_per_output_channel=True)\n",
    "per_channel_quant_conv.quant_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the weight scale is now a vector. Everything we said so far about quantizers still applies, so for example we can load the floating-point state dict we defined before and observe how it triggers an update of the weight scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[ 0.1913,  0.1913, -0.1913],\n",
       "          [-0.1913, -0.1913,  0.1913],\n",
       "          [-0.1913, -0.1913,  0.1913]],\n",
       "\n",
       "         [[-0.1913, -0.1913, -0.1913],\n",
       "          [ 0.1913,  0.1913,  0.1913],\n",
       "          [-0.1913,  0.1913,  0.1913]],\n",
       "\n",
       "         [[ 0.1913,  0.1913,  0.1913],\n",
       "          [-0.1913,  0.1913, -0.1913],\n",
       "          [-0.1913, -0.1913,  0.1913]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1889,  0.1889,  0.1889],\n",
       "          [ 0.1889,  0.1889,  0.1889],\n",
       "          [-0.1889, -0.1889, -0.1889]],\n",
       "\n",
       "         [[-0.1889, -0.1889,  0.1889],\n",
       "          [-0.1889,  0.1889, -0.1889],\n",
       "          [-0.1889,  0.1889, -0.1889]],\n",
       "\n",
       "         [[-0.1889, -0.1889, -0.1889],\n",
       "          [ 0.1889, -0.1889, -0.1889],\n",
       "          [-0.1889,  0.1889,  0.1889]]]], grad_fn=<MulBackward0>), scale=tensor([[[[0.1913]]],\n",
       "\n",
       "\n",
       "        [[[0.1889]]]], grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed=True, training=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_channel_quant_conv.load_state_dict(float_conv.state_dict())\n",
    "per_channel_quant_conv.quant_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have a per-channel quantizer, so the original floating-point weight tensor is now quantized per channel. \n",
    "\n",
    "Similarly, we can apply our custom activation quantizer to e.g. a `QuantIdentity` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0100, -0.0100,  0.0100,  0.0100],\n",
       "        [-0.0100,  0.0100, -0.0100,  0.0100],\n",
       "        [ 0.0100,  0.0100,  0.0100,  0.0100],\n",
       "        [-0.0100,  0.0100, -0.0100,  0.0100]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "quant_identity = QuantIdentity(\n",
    "    act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=False)\n",
    "quant_identity(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how `AdvancedActQuantizer` doesn't define a `per_channel_broadcastable_shape`, yet no errors are triggered. This is because `this.per_channel_broadcastable_shape` is required only when `scaling_per_output_channel` is `True`, while in this case `scaling_per_output_channel` is `False`. Let' try that then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "DependencyError",
     "evalue": "'AdvancedActQuantizer' can not resolve attribute 'per_channel_broadcastable_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDependencyError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-b3479e90d1a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbrevitas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQuantIdentity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m quant_identity = QuantIdentity(\n\u001b[0m\u001b[0;32m      4\u001b[0m     act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=True)\n",
      "\u001b[1;32mc:\\brevitas\\src\\brevitas\\nn\\quant_activation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, act_quant, return_quant_tensor, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mreturn_quant_tensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             **kwargs):\n\u001b[1;32m--> 130\u001b[1;33m         QuantNLAL.__init__(\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0minput_quant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\brevitas\\src\\brevitas\\nn\\quant_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, act_impl, passthrough_act, input_quant, act_quant, return_quant_tensor, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mQuantLayerMixin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_quant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mQuantInputMixin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_quant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         QuantNonLinearActMixin.__init__(\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mact_impl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\brevitas\\src\\brevitas\\nn\\mixin\\act.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, act_impl, passthrough_act, act_quant, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mact_quant\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mActQuantProxyProtocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mType\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInjector\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             **kwargs):\n\u001b[1;32m--> 184\u001b[1;33m         QuantActMixin.__init__(\n\u001b[0m\u001b[0;32m    185\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mact_impl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mact_impl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\brevitas\\src\\brevitas\\nn\\mixin\\act.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, act_impl, passthrough_act, act_quant, proxy_from_injector_impl, proxy_prefix, kwargs_prefix, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[0mact_quant_injector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mact_quant_injector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_impl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mact_impl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mact_quant_injector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_aqi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_quant_injector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mact_quant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxy_from_injector_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_quant_injector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_quant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActQuantProxyProtocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\brevitas\\src\\brevitas\\proxy\\runtime_quant.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, act_quant_injector)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_quant_injector\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mInjector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActQuantProxyFromInjector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_quant_injector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mtensor_quant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mact_quant_injector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_quant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mact_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mact_quant_injector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_impl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_quant_enabled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_quant\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\pytorch_latest\\lib\\site-packages\\_dependencies\\this.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, __self__)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\".\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mDependencyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                     message = (\n",
      "\u001b[1;32mc:\\brevitas\\src\\brevitas\\inject\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(cls, attrname)\u001b[0m\n\u001b[0;32m    115\u001b[0m                         \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_attr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                     )\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mDependencyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mmarker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhave_defaults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDependencyError\u001b[0m: 'AdvancedActQuantizer' can not resolve attribute 'per_channel_broadcastable_shape'"
     ]
    }
   ],
   "source": [
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "quant_identity = QuantIdentity(\n",
    "    act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we get an error saying that the quantizer cannot resolve `per_channel_broadcastable_shape`. If we pass it in then we can get a per-channel quantizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[ 0.0100, -0.0100,  0.0100,  0.0100],\n",
       "        [-0.0100, -0.0100,  0.0100,  0.0100],\n",
       "        [-0.0100, -0.0100,  0.0100,  0.0100],\n",
       "        [ 0.0100, -0.0100,  0.0100,  0.0100]], grad_fn=<MulBackward0>), scale=tensor([[0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100]], grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed=True, training=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_identity = QuantIdentity(\n",
    "    act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=True,\n",
    "    per_channel_broadcastable_shape=(4, 1), return_quant_tensor=True)\n",
    "quant_identity(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how powerful dependency injection is. In a way, it's even too expressive. For users that are not interesting in building completely custom quantizers it's a bit too much. In particular, it can be hard to make sense of how the various components available under `brevitas.core` can be assembled together according to be practices. \n",
    "\n",
    "In the next tutorial then we are going to take a look at an API implemented with the mechanisms we just saw that builds on top of the components available in `brevitas.core` and exposes easily switchable settings that follows best practices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
