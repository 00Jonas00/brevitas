

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Brevitas &mdash; Brevitas 0.1.0-alpha documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="#" class="icon icon-home"> Brevitas
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Brevitas</a><ul>
<li><a class="reference internal" href="#requirements">Requirements</a></li>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#target-audience">Target audience</a></li>
<li><a class="reference internal" href="#installation">Installation</a><ul>
<li><a class="reference internal" href="#installing-from-master">Installing from master</a></li>
<li><a class="reference internal" href="#dev-install">Dev install</a></li>
</ul>
</li>
<li><a class="reference internal" href="#features">Features</a></li>
<li><a class="reference internal" href="#supported-layers">Supported Layers</a></li>
<li><a class="reference internal" href="#getting-started">Getting started</a></li>
<li><a class="reference internal" href="#code-organization">Code organization</a></li>
<li><a class="reference internal" href="#a-note-on-the-implementation">A note on the implementation</a></li>
<li><a class="reference internal" href="#author">Author</a></li>
</ul>
</li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Brevitas</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li>Brevitas</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="toctree-wrapper compound">
</div>
<div class="section" id="brevitas">
<h1>Brevitas<a class="headerlink" href="#brevitas" title="Permalink to this headline">¶</a></h1>
<a class="reference external image-reference" href="https://zenodo.org/badge/latestdoi/140494324"><img alt="DOI" src="https://zenodo.org/badge/140494324.svg" /></a>
<p>Brevitas is a Pytorch library for training-aware quantization.</p>
<p><em>Brevitas is currently under active development and to be considered in alpha stage. APIs might and probably will change. Documentation, examples, and pretrained models will be progressively released.</em></p>
<div class="section" id="requirements">
<h2>Requirements<a class="headerlink" href="#requirements" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org">Pytorch</a> &gt;= 1.1.0</p></li>
</ul>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Brevitas implements a set of building blocks to model a reduced precision hardware data-path at training time.
While partially biased towards modelling dataflow-style, very low-precision implementations, the building blocks can be parametrized and assembled together to target all sorts of reduced precision hardware.</p>
<p>The implementations tries to adhere to the following design principles:</p>
<ul class="simple">
<li><p>Idiomatic Pytorch, when possible.</p></li>
<li><p>Modularity first, at the cost of some verbosity.</p></li>
<li><p>Easily extendible.</p></li>
</ul>
</div>
<div class="section" id="target-audience">
<h2>Target audience<a class="headerlink" href="#target-audience" title="Permalink to this headline">¶</a></h2>
<p>Brevitas is mainly targeted at researchers and practicioners in the fields of training for reduced precision inference.</p>
<p>The implementation is quite rich in options and allows for very fine grained control over the trained model.
However, compared to other software solutions in this space, the burden of correctly modelling the target data-path is currently placed on the user.</p>
</div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="installing-from-master">
<h3>Installing from master<a class="headerlink" href="#installing-from-master" title="Permalink to this headline">¶</a></h3>
<p>You can install the latest master directly from GitHub:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install git+https://github.com/Xilinx/brevitas.git
</pre></div>
</div>
</div>
<div class="section" id="dev-install">
<h3>Dev install<a class="headerlink" href="#dev-install" title="Permalink to this headline">¶</a></h3>
<p>Alternatively, you can install a dev copy straight from the cloned repo:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/Xilinx/brevitas.git
<span class="nb">cd</span> brevitas
pip install -e .
</pre></div>
</div>
</div>
</div>
<div class="section" id="features">
<h2>Features<a class="headerlink" href="#features" title="Permalink to this headline">¶</a></h2>
<p>Brevitas’ features are organized along the following (mostly) orthogonal axes:</p>
<ul class="simple">
<li><p><strong>Quantization type</strong>: binary, ternary, or uniform integer quantization.</p></li>
<li><p><strong>Scaling</strong>: support for various shapes, learning strategies and constraints.</p></li>
<li><p><strong>Precision</strong>: constant or learned bit-width.</p></li>
<li><p><strong>Target tensor</strong>: weights, activations or accumulators.</p></li>
<li><p><strong>Cost</strong>: model the hardware cost at training-time.</p></li>
</ul>
</div>
<div class="section" id="supported-layers">
<h2>Supported Layers<a class="headerlink" href="#supported-layers" title="Permalink to this headline">¶</a></h2>
<p>The following layers and operations are supported out-of-the-box:</p>
<ul class="simple">
<li><p>QuantLinear</p></li>
<li><p>QuantConv2d</p></li>
<li><p>QuantReLU, QuantHardTanh, QuantTanh, QuantSigmoid</p></li>
<li><p>QuantAvgPool2d</p></li>
<li><p>QuantBatchNorm2d</p></li>
<li><p>Element-wise add, concat</p></li>
<li><p>Saturating integer accumulator</p></li>
</ul>
<p>Additional layers can be easily supported using a combination of pre-existing modules.</p>
</div>
<div class="section" id="getting-started">
<h2>Getting started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h2>
<p>Here’s how a simple quantized LeNet might look like, starting from a ReLU6 for activations and using default settings for scaling:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">brevitas.nn</span> <span class="kn">as</span> <span class="nn">qnn</span>
<span class="kn">from</span> <span class="nn">brevitas.core.quant</span> <span class="kn">import</span> <span class="n">QuantType</span>

<span class="k">class</span> <span class="nc">QuantLeNet</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">QuantLeNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">qnn</span><span class="o">.</span><span class="n">QuantConv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span>
                                     <span class="n">weight_quant_type</span><span class="o">=</span><span class="n">QuantType</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
                                     <span class="n">weight_bit_width</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span> <span class="o">=</span> <span class="n">qnn</span><span class="o">.</span><span class="n">QuantReLU</span><span class="p">(</span><span class="n">quant_type</span><span class="o">=</span><span class="n">QuantType</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span> <span class="n">bit_width</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">qnn</span><span class="o">.</span><span class="n">QuantConv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span>
                                     <span class="n">weight_quant_type</span><span class="o">=</span><span class="n">QuantType</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
                                     <span class="n">weight_bit_width</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu2</span> <span class="o">=</span> <span class="n">qnn</span><span class="o">.</span><span class="n">QuantReLU</span><span class="p">(</span><span class="n">quant_type</span><span class="o">=</span><span class="n">QuantType</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span> <span class="n">bit_width</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span>   <span class="o">=</span> <span class="n">qnn</span><span class="o">.</span><span class="n">QuantLinear</span><span class="p">(</span><span class="mi">16</span><span class="o">*</span><span class="mi">5</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                     <span class="n">weight_quant_type</span><span class="o">=</span><span class="n">QuantType</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
                                     <span class="n">weight_bit_width</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu3</span> <span class="o">=</span> <span class="n">qnn</span><span class="o">.</span><span class="n">QuantReLU</span><span class="p">(</span><span class="n">quant_type</span><span class="o">=</span><span class="n">QuantType</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span> <span class="n">bit_width</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span>   <span class="o">=</span> <span class="n">qnn</span><span class="o">.</span><span class="n">QuantLinear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                     <span class="n">weight_quant_type</span><span class="o">=</span><span class="n">QuantType</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
                                     <span class="n">weight_bit_width</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu4</span> <span class="o">=</span> <span class="n">qnn</span><span class="o">.</span><span class="n">QuantReLU</span><span class="p">(</span><span class="n">quant_type</span><span class="o">=</span><span class="n">QuantType</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span> <span class="n">bit_width</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span>   <span class="o">=</span> <span class="n">qnn</span><span class="o">.</span><span class="n">QuantLinear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                     <span class="n">weight_quant_type</span><span class="o">=</span><span class="n">QuantType</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
                                     <span class="n">weight_bit_width</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu4</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
<div class="section" id="code-organization">
<h2>Code organization<a class="headerlink" href="#code-organization" title="Permalink to this headline">¶</a></h2>
<p>The implementation in organized in three different levels, each corresponding to a package:</p>
<ul class="simple">
<li><p><strong>core</strong>: contains the implementation of all Brevitas’ quantization routines, leveraging the <em>JIT</em> as much as possible.</p></li>
<li><p><strong>proxy</strong>: combines the routines implemented in the core in way that makes sense for different target tensors, namely weights, activations and accumulators.</p></li>
<li><p><strong>nn</strong>: wraps the different proxies in a user-facing API, which can be used as a drop-in replacement for torch.nn modules.</p></li>
</ul>
<p>Additionally, the following packages are present:</p>
<ul class="simple">
<li><p><strong>function</strong>: implements various support routines.</p></li>
<li><p><strong>cost</strong>: exposes different loss functions for modelling the hardware cost at training-time or for regularization purposes.</p></li>
</ul>
</div>
<div class="section" id="a-note-on-the-implementation">
<h2>A note on the implementation<a class="headerlink" href="#a-note-on-the-implementation" title="Permalink to this headline">¶</a></h2>
<p>Brevitas operates (possibly but not strictly) on a <em>QuantTensor</em>, currently implemented as a <em>NamedTuple</em> because of a lack of support for custom Tensor sub-classes in Pytorch. This might change in the future.</p>
<p>A QuantTensor propagates the following information:</p>
<ul class="simple">
<li><p><strong>quant_tensor</strong>: the quantized tensor, in dequantized representation (i.e. floating-point order of magnitude).</p></li>
<li><p><strong>scale_factor</strong>: the scale factor implicit in <em>quant_tensor</em>.</p></li>
<li><p><strong>bit_width</strong>: the precision of <em>quant_tensor</em> in bits.</p></li>
</ul>
<p>Propagating scale factors and bit-width along the forward pass allows to model and operate on accumulators, whose bit-width and scale factor depends on the layers contributing to them.
However, propagating a tuple of values in a forward() call breaks compatibility with certain modules like <em>nn.Sequential</em>, which assumes 1 input and 1 output. As such, operating on a QuantTensor is optional.</p>
</div>
<div class="section" id="author">
<h2>Author<a class="headerlink" href="#author" title="Permalink to this headline">¶</a></h2>
<p>Alessandro Pappalardo &#64; Xilinx Research Labs.</p>
</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Alessandro Pappalardo

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>